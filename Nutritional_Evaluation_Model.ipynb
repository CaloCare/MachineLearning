{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaloCare/MachineLearning/blob/main/Nutritional_Evaluation_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CaloCare"
      ],
      "metadata": {
        "id": "3_RYfB2Qe4e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Packages/Libraries"
      ],
      "metadata": {
        "id": "nIO8ZhJZexAj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tLsDxZazpj40"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for data manipulation and machine learning\n",
        "import pandas as pd  # For handling datasets\n",
        "from sklearn.model_selection import train_test_split  # For splitting datasets\n",
        "import tensorflow as tf  # For building and training neural networks\n",
        "import numpy as np  # For numerical computations\n",
        "import os  # For interacting with the operating system"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Keras Tuner for hyperparameter optimization\n",
        "!pip install keras-tuner --upgrade\n",
        "# Install tensorflow\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoHP4AWIquJm",
        "outputId": "3c1376ad-5a87-47e0-d2c4-dc353512158f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules for building and tuning neural network models\n",
        "from tensorflow.keras.models import Sequential  # For creating sequential neural network models\n",
        "from tensorflow.keras.layers import Dense  # For adding dense layers to the model\n",
        "from kerastuner.tuners import RandomSearch  # For performing random search hyperparameter tuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKe3AH6sqoZ_",
        "outputId": "711df28a-2af3-4edf-ae89-ee622cf27667"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-940bdc4f5542>:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch  # For performing random search hyperparameter tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**\n",
        "\n",
        "This code sets up the environment for building and optimizing a machine learning model using TensorFlow and Keras. It imports essential libraries such as Pandas for data manipulation, Scikit-learn for dataset splitting, and TensorFlow for constructing and training neural networks. The Keras Tuner is also installed to facilitate hyperparameter optimization, specifically using RandomSearch to find the best model configuration. The purpose of this setup is to allow for efficient model tuning, ultimately leading to improved performance on the given dataset."
      ],
      "metadata": {
        "id": "S4kr1ym6fZ86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling"
      ],
      "metadata": {
        "id": "H1BJPWRlqXJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle CLI for downloading datasets\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqnUbyZRqWUX",
        "outputId": "41252e60-c4be-45d9-e29b-f712a87cac47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Diet2Ne8rJCr",
        "outputId": "57d09028-dc3a-41e2-f452-6c0bdaf2fe43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Indonesian food and drink nutrition dataset from Kaggle\n",
        "!kaggle datasets download -d anasfikrihanif/indonesian-food-and-drink-nutrition-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfQzcxeKrat-",
        "outputId": "644ca69b-0ba3-4ddd-d193-09b38cd42e1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/anasfikrihanif/indonesian-food-and-drink-nutrition-dataset\n",
            "License(s): CC0-1.0\n",
            "indonesian-food-and-drink-nutrition-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the downloaded dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('indonesian-food-and-drink-nutrition-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('indonesian-food-and-drink-nutrition-dataset')  # Extract all files to the specified folder\n",
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "data = pd.read_csv('/content/indonesian-food-and-drink-nutrition-dataset/nutrition.csv')  # Read the CSV file"
      ],
      "metadata": {
        "id": "XJ6q29fgrdMu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**\n",
        "\n",
        "This code demonstrates how to download and extract the Indonesian food and drink nutrition dataset from Kaggle using the `Kaggle CLI`. After installing the necessary packages and configuring the Kaggle API key, the dataset is downloaded and unzipped. The CSV file containing the nutritional data is then loaded into a Pandas DataFrame for further analysis. This process sets up the dataset for potential machine learning tasks, such as building predictive models or performing exploratory data analysis."
      ],
      "metadata": {
        "id": "pkTi7aLtfnRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting Data"
      ],
      "metadata": {
        "id": "ykEb0SBzrydG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total nutrition by summing calories, proteins, fat, and carbohydrates\n",
        "data['total_nutrition'] = data['calories'] + data['proteins'] + data['fat'] + data['carbohydrate']\n",
        "\n",
        "# Create nutrition categories based on the total nutrition value\n",
        "# Define bins and labels for categorization\n",
        "bins = [0, 200, 400, 600, 800, np.inf]  # Define the nutrition range intervals\n",
        "labels = [1, 2, 3, 4, 5]  # Assign labels for each bin\n",
        "data['evaluation'] = pd.cut(data['total_nutrition'], bins=bins, labels=labels)  # Categorize based on total nutrition\n",
        "\n",
        "# Convert the 'evaluation' column to numeric values (integer codes)\n",
        "data['evaluation'] = data['evaluation'].cat.codes  # Convert categorical labels to integer codes\n",
        "\n",
        "# Define the feature set (X) and target variable (y)\n",
        "# In this case, the target is 'evaluation' and the features are nutritional components\n",
        "X = data[['calories', 'proteins', 'fat', 'carbohydrate']]  # Features (independent variables)\n",
        "y = data['evaluation']  # Target variable (dependent variable)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "# 80% for training and 20% for validation, ensuring random splitting with a fixed seed (random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  # Split the data"
      ],
      "metadata": {
        "id": "hK-tuehlrxfW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**\n",
        "\n",
        "This code processes the nutritional data by calculating the total nutrition for each food item, which is the sum of calories, proteins, fat, and carbohydrates. Based on the total nutrition value, the data is categorized into five groups using defined bins and labels, and the 'evaluation' column is converted into numeric values for modeling purposes. The features (calories, proteins, fat, and carbohydrates) are separated from the target variable ('evaluation'), which represents the nutrition category. Finally, the dataset is split into training and validation sets `(80% for training, 20% for validation)`, preparing it for machine learning model development."
      ],
      "metadata": {
        "id": "D2nlimRFf3AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyper-Parameter Tuning"
      ],
      "metadata": {
        "id": "mr7Dvaa6r8H-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to build the model with hyperparameter tuning using Keras Tuner\n",
        "def build_model(hp):\n",
        "    # Initialize a Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add the first dense layer with a tunable number of units\n",
        "    # The number of units is selected from a range (32 to 512) with a step size of 32\n",
        "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "                    activation='relu',  # Use ReLU activation function\n",
        "                    input_shape=(X_train.shape[1],)))  # Define the input shape based on training data\n",
        "\n",
        "    # Add the output layer with 5 units, corresponding to the 5 classes in the target variable\n",
        "    model.add(Dense(5, activation='softmax'))  # Softmax activation for multi-class classification\n",
        "\n",
        "    # Compile the model with the Adam optimizer and sparse categorical cross-entropy loss\n",
        "    # Sparse categorical cross-entropy is used as labels are integers, not one-hot encoded\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])  # Use accuracy as the evaluation metric\n",
        "\n",
        "    # Return the compiled model\n",
        "    return model"
      ],
      "metadata": {
        "id": "TbbSxpL_r-ru"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**\n",
        "\n",
        "This function defines a neural network model using Keras Tuner for hyperparameter optimization. It creates a Sequential model with a tunable number of units in the first dense layer, allowing the number of neurons to be chosen from a specified range `(32 to 512)`. The model uses the `ReLU activation` function for the hidden layer and `softmax` for the output layer, suitable for multi-class classification. The model is compiled with the `Adam optimizer and sparse categorical cross-entropy loss`, as the target labels are integers rather than one-hot encoded. This setup is designed to facilitate hyperparameter tuning to find the best configuration for the model."
      ],
      "metadata": {
        "id": "BggVFEqsgEWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RandomSearch tuner from Keras Tuner to optimize the hyperparameters\n",
        "tuner = RandomSearch(\n",
        "    build_model,  # The function used to build the model\n",
        "    objective='val_accuracy',  # The objective to optimize is validation accuracy\n",
        "    max_trials=10,  # Set the maximum number of trials (experiments)\n",
        "    executions_per_trial=3,  # Number of executions per trial to average out results\n",
        "    directory='my_dir',  # Directory to store the tuner logs and results\n",
        "    project_name='hyperparam_tuning'  # Name of the project (for organization purposes)\n",
        ")\n",
        "\n",
        "# Display the summary of the search space (hyperparameters) to be tuned\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Start the hyperparameter search with the training data (X_train, y_train)\n",
        "# The search will run for 50 epochs and use the validation data (X_val, y_val) for evaluation\n",
        "tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
        "\n",
        "# Display the summary of the tuner results after the search\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33QVvELUsAmu",
        "outputId": "70f2d2eb-db23-4d3d-9316-6457958b5baa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 29s]\n",
            "val_accuracy: 0.9012345671653748\n",
            "\n",
            "Best val_accuracy So Far: 0.9012345671653748\n",
            "Total elapsed time: 00h 06m 07s\n",
            "Results summary\n",
            "Results in my_dir/hyperparam_tuning\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "Score: 0.9012345671653748\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "Score: 0.895061731338501\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "Score: 0.89012344678243\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "Score: 0.8888888955116272\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "Score: 0.8876543243726095\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "Score: 0.8864197532335917\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "Score: 0.8802469174067179\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "units: 96\n",
            "Score: 0.8753086527188619\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "Score: 0.8728395104408264\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "Score: 0.814814825852712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best model from the tuner after hyperparameter search\n",
        "# Get the best model based on the highest validation accuracy\n",
        "best_model = tuner.get_best_models(num_models=1)[0]  # Select the best model (top 1)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "# The evaluate method returns the loss and accuracy metrics\n",
        "val_loss, val_acc = best_model.evaluate(X_val, y_val)\n",
        "\n",
        "# Print the evaluation results (loss and accuracy) on the validation set\n",
        "print(f'Validation Loss: {val_loss}')  # Display the validation loss\n",
        "print(f'Validation Accuracy: {val_acc}')  # Display the validation accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOiIj5N4sCX2",
        "outputId": "bf51afca-48fb-4b0a-c37e-5f0c20efec43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9328 - loss: 0.3083\n",
            "Validation Loss: 0.34088724851608276\n",
            "Validation Accuracy: 0.9222221970558167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**\n",
        "\n",
        "This code leverages Keras Tuner's `RandomSearch` to optimize the hyperparameters of a neural network model. It defines the objective as maximizing validation accuracy and runs a total of `10 trials`, with `3 executions per trial` to ensure robust results. The tuner explores different configurations of the model, including the number of units in the dense layer, and evaluates performance over `50 epochs` using the validation data. After completing the hyperparameter search, the best model is selected based on the highest validation accuracy and evaluated on the validation set to report the loss and accuracy metrics. This process helps find the optimal model configuration for better performance."
      ],
      "metadata": {
        "id": "ea85EHXfgeZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n"
      ],
      "metadata": {
        "id": "ISbsPlDIvWlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the evaluation category based on nutritional input\n",
        "def predict_evaluation(calories, proteins, fat, carbohydrate):\n",
        "    # Format the user input as an array that can be passed to the model for prediction\n",
        "    input_data = np.array([[calories, proteins, fat, carbohydrate]])\n",
        "\n",
        "    # Make a prediction using the trained model\n",
        "    prediction = best_model.predict(input_data)\n",
        "\n",
        "    # Convert the prediction to the corresponding evaluation category\n",
        "    # The model output is a probability distribution, so we select the class with the highest probability\n",
        "    predicted_category = np.argmax(prediction, axis=1)[0] + 1  # Add 1 because categories start from 1\n",
        "\n",
        "    return predicted_category  # Return the predicted evaluation category"
      ],
      "metadata": {
        "id": "jm7FF0WjvWM7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight**\n",
        "\n",
        "This function takes user inputs for nutritional values (calories, proteins, fat, and carbohydrates), formats them into an array suitable for model prediction, and uses the trained model to predict the evaluation category. The model's output is a probability distribution, and the function selects the class with the highest probability using `np.argmax`. The result is adjusted by adding 1 because the evaluation categories are indexed starting from 1, and the predicted category is returned as the final output. This function allows for real-time nutritional evaluation based on user input."
      ],
      "metadata": {
        "id": "cgF9pseJhLgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to take user input for nutritional values and predict the evaluation category\n",
        "def user_input():\n",
        "    print(\"Masukkan nilai-nilai nutrisi untuk mendapatkan evaluasi:\")  # Prompt the user to input nutritional values\n",
        "\n",
        "    # Ask the user to input values for calories, proteins, fat, and carbohydrates\n",
        "    calories = float(input(\"Kalori (cal): \"))  # Input calories\n",
        "    proteins = float(input(\"Protein (g): \"))  # Input protein value\n",
        "    fat = float(input(\"Lemak (g): \"))  # Input fat value\n",
        "    carbohydrate = float(input(\"Karbohidrat (g): \"))  # Input carbohydrate value\n",
        "\n",
        "    # Predict the evaluation category based on the provided nutritional values\n",
        "    evaluation = predict_evaluation(calories, proteins, fat, carbohydrate)\n",
        "\n",
        "    # Display the predicted evaluation category\n",
        "    print(f'Nilai evaluasi untuk nutrisi yang diberikan adalah: {evaluation}')  # Output the evaluation result\n",
        "\n",
        "# Call the user_input function to execute the prediction based on user input\n",
        "user_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_foDxAaPvcFe",
        "outputId": "21e8b972-f475-4769-b3ae-b3f87c891cc5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan nilai-nilai nutrisi untuk mendapatkan evaluasi:\n",
            "Kalori (cal): 513\n",
            "Protein (g): 23.7\n",
            "Lemak (g): 37\n",
            "Karbohidrat (g): 21.3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "Nilai evaluasi untuk nutrisi yang diberikan adalah: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to take user input for nutritional values and predict the evaluation category with a description\n",
        "def user_input():\n",
        "    print(\"Masukkan nilai-nilai nutrisi untuk mendapatkan evaluasi:\")  # Prompt the user to input nutritional values\n",
        "\n",
        "    # Ask the user to input values for calories, proteins, fat, and carbohydrates\n",
        "    calories = float(input(\"Kalori (cal): \"))  # Input calories\n",
        "    proteins = float(input(\"Protein (g): \"))  # Input protein value\n",
        "    fat = float(input(\"Lemak (g): \"))  # Input fat value\n",
        "    carbohydrate = float(input(\"Karbohidrat (g): \"))  # Input carbohydrate value\n",
        "\n",
        "    # Predict the evaluation category based on the provided nutritional values\n",
        "    evaluation = predict_evaluation(calories, proteins, fat, carbohydrate)\n",
        "\n",
        "    # Dictionary containing descriptions for each evaluation category\n",
        "    descriptions = {\n",
        "        1: \"buruk untuk Anda\",  # Description for category 1\n",
        "        2: \"tidak terlalu baik untuk Anda\",  # Description for category 2\n",
        "        3: \"cukup baik untuk Anda\",  # Description for category 3\n",
        "        4: \"baik untuk Anda\",  # Description for category 4\n",
        "        5: \"sangat baik untuk Anda\"  # Description for category 5\n",
        "    }\n",
        "\n",
        "    # Display the predicted evaluation category with its description\n",
        "    print(f'Nilai evaluasi untuk nutrisi yang diberikan adalah: {evaluation} ({descriptions[evaluation]})')\n",
        "\n",
        "# Call the user_input function to execute the prediction based on user input\n",
        "user_input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xEiECL6wO5W",
        "outputId": "d54560af-9a85-4c9a-e43b-3e24ab48ab5c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan nilai-nilai nutrisi untuk mendapatkan evaluasi:\n",
            "Kalori (cal): 513\n",
            "Protein (g): 23.7\n",
            "Lemak (g): 37\n",
            "Karbohidrat (g): 21.23\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Nilai evaluasi untuk nutrisi yang diberikan adalah: 4 (baik untuk Anda)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saved Model to TFlite"
      ],
      "metadata": {
        "id": "s0mj1U92EctU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your saved model is in a directory named 'nutrition_model'\n",
        "export_dir = 'saved_model/1'\n",
        "tf.saved_model.save(best_model, export_dir)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Ensure FLOAT32 input type\n",
        "converter.target_spec.supported_types = [tf.float32]\n",
        "\n",
        "# Add representative dataset if quantization is applied\n",
        "def representative_data_gen():\n",
        "    for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n",
        "        yield [input_value.astype(np.float32)]\n",
        "\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Convert to TFLite\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted TFLite model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Model successfully converted to TFLite.\")"
      ],
      "metadata": {
        "id": "Rn-7rZPaEqY9",
        "outputId": "ffe3639e-9816-4948-a605-801154f1fde7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully converted to TFLite.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "UdK5dcp1e97n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a system for predicting the nutritional evaluation of food based on user input. It first collects values for calories, proteins, fat, and carbohydrates, and then predicts the evaluation category using a trained model. The model outputs a category, which is enhanced with a description indicating whether the nutritional values are good or bad for the user. The validation results show a strong model performance with a `validation loss of 0.34 and an accuracy of 92%`, indicating that the model is effective in categorizing the nutritional data."
      ],
      "metadata": {
        "id": "2S3HGRFJhRDp"
      }
    }
  ]
}